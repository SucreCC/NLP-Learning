# 自然语言处理中的词嵌入（Word Embedding）介绍

## 目录
- [1. 什么是词嵌入(Word Embedding)](#1-什么是词嵌入word-embedding)
- [2. 离散表示](#2-离散表示)
  - [2.1 One-hot表示](#21-one-hot表示)
  - [2.2 词袋模型](#22-词袋模型)
  - [2.3 TF-IDF](#23-tf-idf)
  - [2.4 n-gram模型](#24-n-gram模型)
  - [2.5 离散表示存在的问题](#25-离散表示存在的问题)
- [3. 分布式表示](#3-分布式表示)
  - [3.1 共现矩阵](#31-共现矩阵)
- [4. 神经网络表示](#4-神经网络表示)
  - [4.1 NNLM](#41-nnlm)
  - [4.2 Word2Vec](#42-word2vec)
  - [4.3 GloVe](#43-glove)
  - [4.4 ELMo](#44-elmo)
  - [4.5 BERT](#45-bert)
  - [4.6 GPT](#46-gpt)
- [5. 词嵌入为何不采用one-hot向量](#5-词嵌入为何不采用one-hot向量)

---

## 1. 什么是词嵌入(Word Embedding)

自然语言处理（NLP）中的词嵌入是将词语映射为实数域向量的技术。近年来，词嵌入已成为自然语言处理的基础。词嵌入将词语的语义转化为向量，使计算机能够理解和操作文本。

### 文本表示的演变：
- **早期**：基于规则的方法来表示文本。
- **现代**：基于统计机器学习的方法，如词嵌入技术。

文本表示包括 **离散表示** 和 **分布式表示**。

---

## 2. 离散表示

### 2.1 One-hot表示
通过创建一个大词典并为每个单词分配一个唯一的向量表示。该向量除了一个位置为1，其他全为0。  
**缺点**：无法捕捉词与词之间的关系，导致高维稀疏矩阵。

### 2.2 词袋模型（Bag of Words）
不考虑词的顺序，仅关注每个词的出现次数。  
**缺点**：忽略了词序信息和词与词之间的语义关系。

### 2.3 TF-IDF
通过考虑词在文档中出现的频率和在整个语料库中出现的频率，衡量词的重要性。  
**缺点**：仍无法捕捉词的顺序和上下文信息。

### 2.4 n-gram模型
通过考虑固定窗口大小的词组来保持词序信息。  
**缺点**：词典随着n值的增加而快速膨胀。

### 2.5 离散表示存在的问题
- 无法衡量词与词之间的关系。
- 随着语料库的增大，维度爆炸，导致稀疏矩阵。
- 丢失语义信息，模型欠稳定。

---

## 3. 分布式表示

### 3.1 共现矩阵
通过统计词在特定窗口内共同出现的次数来表示词与词之间的关系。  
**存在的问题**：
- 向量维度随词典大小线性增长。
- 需要大量存储空间，导致稀疏性问题。

---

## 4. 神经网络表示

### 4.1 NNLM（Neural Network Language Model）
神经网络语言模型，通过训练得到词向量矩阵。NNLM 定义了一个窗口，通过预测最后一个词的概率来训练。

### 4.2 Word2Vec
谷歌于 2013 年提出的 Word2Vec 是一种浅层神经网络模型，有两种结构：
- **CBOW**：根据上下文预测中心词。
- **Skip-gram**：根据中心词预测上下文。
  
**优化方法**：
- **层次Softmax**：通过 Huffman Tree 降低计算复杂度。
- **负例采样**：减少负样本数量，降低计算量。

**存在问题**：无法处理多义词。

### 4.3 GloVe（Global Vectors for Word Representation）
GloVe 是一种基于共现矩阵的词嵌入方法，由斯坦福大学于 2014 年提出。GloVe 将词与词之间的共现信息编码为稠密向量，并通过矩阵分解的方式进行优化。

**特点**：
- GloVe 模型捕捉了全局语料库的统计信息，比 Word2Vec 更适合描述词语的全局共现关系。
- GloVe 通过对共现矩阵的优化，保留了词与词之间的距离和方向关系。

### 4.4 ELMo（Embeddings from Language Models）
ELMo 由 AllenNLP 团队于 2018 年提出，是一种基于上下文的词嵌入模型。与静态词嵌入方法不同，ELMo 可以根据上下文生成动态的词表示。

**特点**：
- ELMo 使用双向 LSTM（BiLSTM），可以捕捉到句子中的前后语境。
- 词的嵌入向量会根据不同的上下文变化，因此能够处理多义词。

### 4.5 BERT（Bidirectional Encoder Representations from Transformers）
BERT 是 Google 于 2018 年提出的革命性预训练语言模型，基于 Transformer 架构。它通过双向编码器捕捉上下文信息，并使用无监督的 Masked Language Model 进行预训练。

**特点**：
- BERT 是双向模型，能够同时理解句子的前后文。
- BERT 是预训练模型，可以通过微调适应不同的下游任务（如分类、问答、文本生成）。
- BERT 生成的词向量是动态的，能够捕捉到语境中的不同含义。

### 4.6 GPT（Generative Pre-trained Transformer）
GPT 是 OpenAI 于 2018 年提出的生成式预训练语言模型，使用自回归（Autoregressive）方法进行文本生成。

**特点**：
- GPT 使用 Transformer 的解码器部分，并通过无监督学习进行大规模预训练。
- GPT 可以用于生成任务（如对话系统、文本生成）。
- 最新的 GPT-3 和 GPT-4 拥有数十亿到上千亿参数，进一步提升了生成质量。

### 4.7 现代预训练模型的比较
- **Word2Vec 和 GloVe** 是静态词嵌入模型，无法根据上下文动态调整词的表示。
- **ELMo、BERT 和 GPT** 是基于上下文的动态嵌入模型，能够根据语境生成不同的词表示。
- **BERT** 专注于理解任务，而 **GPT** 更专注于生成任务。

---

## 5. 词嵌入为何不采用One-hot向量

One-hot向量无法准确表达不同词之间的相似度。Word2Vec 通过学习词向量解决了这个问题，使得不同词之间的相似性和类比关系可以通过向量表示。

