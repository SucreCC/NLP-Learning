# 自然语言处理中的词嵌入（Word Embedding）介绍

## 目录
- [1. 什么是词嵌入(Word Embedding)](#1-什么是词嵌入word-embedding)
- [2. 离散表示](#2-离散表示)
  - [2.1 One-hot表示](#21-one-hot表示)
  - [2.2 词袋模型](#22-词袋模型)
  - [2.3 TF-IDF](#23-tf-idf)
  - [2.4 n-gram模型](#24-n-gram模型)
  - [2.5 离散表示存在的问题](#25-离散表示存在的问题)
- [3. 分布式表示](#3-分布式表示)
  - [3.1 共现矩阵](#31-共现矩阵)
- [4. 神经网络表示](#4-神经网络表示)
  - [4.1 NNLM](#41-nnlm)
  - [4.2 Word2Vec](#42-word2vec)
  - [4.3 Sense2Vec](#43-sense2vec)
- [5. 词嵌入为何不采用one-hot向量](#5-词嵌入为何不采用one-hot向量)

---

## 1. 什么是词嵌入(Word Embedding)

自然语言处理（NLP）中的词嵌入是将词语映射为实数域向量的技术。近年来，词嵌入已成为自然语言处理的基础。词嵌入将词语的语义转化为向量，使计算机能够理解和操作文本。

### 文本表示的演变：
- **早期**：基于规则的方法来表示文本。
- **现代**：基于统计机器学习的方法，如词嵌入技术。

文本表示包括 **离散表示** 和 **分布式表示**。

---

## 2. 离散表示

### 2.1 One-hot表示
通过创建一个大词典并为每个单词分配一个唯一的向量表示。该向量除了一个位置为1，其他全为0。  
**缺点**：无法捕捉词与词之间的关系，导致高维稀疏矩阵。

### 2.2 词袋模型（Bag of Words）
不考虑词的顺序，仅关注每个词的出现次数。  
**缺点**：忽略了词序信息和词与词之间的语义关系。

### 2.3 TF-IDF
通过考虑词在文档中出现的频率和在整个语料库中出现的频率，衡量词的重要性。  
**缺点**：仍无法捕捉词的顺序和上下文信息。

### 2.4 n-gram模型
通过考虑固定窗口大小的词组来保持词序信息。  
**缺点**：词典随着n值的增加而快速膨胀。

### 2.5 离散表示存在的问题
- 无法衡量词与词之间的关系。
- 随着语料库的增大，维度爆炸，导致稀疏矩阵。
- 丢失语义信息，模型欠稳定。

---

## 3. 分布式表示

### 3.1 共现矩阵
通过统计词在特定窗口内共同出现的次数来表示词与词之间的关系。  
**存在的问题**：
- 向量维度随词典大小线性增长。
- 需要大量存储空间，导致稀疏性问题。

---

## 4. 神经网络表示

### 4.1 NNLM（Neural Network Language Model）
神经网络语言模型，通过训练得到词向量矩阵。NNLM 定义了一个窗口，通过预测最后一个词的概率来训练。

### 4.2 Word2Vec
谷歌于 2013 年提出的 Word2Vec 是一种浅层神经网络模型，有两种结构：
- **CBOW**：根据上下文预测中心词。
- **Skip-gram**：根据中心词预测上下文。
  
**优化方法**：
- **层次Softmax**：通过 Huffman Tree 降低计算复杂度。
- **负例采样**：减少负样本数量，降低计算量。

**存在问题**：无法处理多义词。

### 4.3 Sense2Vec
通过为每个词提供上下文信息，解决 Word2Vec 无法处理多义词的问题。

---

## 5. 词嵌入为何不采用One-hot向量

One-hot向量无法准确表达不同词之间的相似度。Word2Vec 通过学习词向量解决了这个问题，使得不同词之间的相似性和类比关系可以通过向量表示。


